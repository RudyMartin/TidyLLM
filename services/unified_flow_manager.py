"""
Unified Flow Manager (UFM)
==========================

Central orchestration layer for managing all workflow types in the TidyLLM ecosystem.
Provides CRUD operations, health monitoring, and integration with RAG systems.

Similar to UnifiedRAGManager but for workflow orchestration.

Architecture: Portal â†’ UnifiedFlowManager â†’ Workflow Orchestrators â†’ Infrastructure
"""

import json
import time
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
import traceback

try:
    from tidyllm.infrastructure.session.unified import UnifiedSessionManager
    from tidyllm.services.unified_rag_manager import UnifiedRAGManager, RAGSystemType
except ImportError as e:
    print(f"Warning: TidyLLM infrastructure not available: {e}")


class WorkflowSystemType(Enum):
    """Enumeration of supported workflow system types."""
    MVR_ANALYSIS = "mvr_analysis"
    DOMAIN_RAG = "domain_rag"
    FINANCIAL_ANALYSIS = "financial_analysis"
    CONTRACT_REVIEW = "contract_review"
    COMPLIANCE_CHECK = "compliance_check"
    QUALITY_CHECK = "quality_check"
    PEER_REVIEW = "peer_review"
    DATA_EXTRACTION = "data_extraction"
    HYBRID_ANALYSIS = "hybrid_analysis"
    CODE_REVIEW = "code_review"
    RESEARCH_SYNTHESIS = "research_synthesis"
    CLASSIFICATION = "classification"
    CUSTOM_WORKFLOW = "custom_workflow"


class WorkflowStatus(Enum):
    """Workflow execution status."""
    CREATED = "created"
    DEPLOYED = "deployed"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    PAUSED = "paused"
    ARCHIVED = "archived"


class UnifiedFlowManager:
    """
    Central orchestration layer for all workflow systems in TidyLLM.

    Provides unified interface for:
    - CRUD operations on workflows
    - Health monitoring and performance tracking
    - RAG system integration
    - Workflow lifecycle management
    """

    def __init__(self, auto_load_credentials: bool = True):
        """Initialize Unified Flow Manager."""
        print("+ Initializing Unified Flow Manager...")

        # Core components
        self.usm = None
        self.rag_manager = None

        # Workflow storage
        self.workflows_dir = Path("tidyllm/workflows/active")
        self.templates_dir = Path("tidyllm/workflows/templates")
        self.registry_file = Path("extracted_files/tidyllm/workflows/definitions/registry.json")

        # Create directories
        self.workflows_dir.mkdir(parents=True, exist_ok=True)
        self.templates_dir.mkdir(parents=True, exist_ok=True)

        # State tracking
        self.workflow_cache = {}
        self.health_cache = {}
        self.last_health_check = {}

        # Load registry
        self.workflow_registry = self._load_workflow_registry()

        # Initialize connections
        if auto_load_credentials:
            self._initialize_connections()

        print("+ Unified Flow Manager initialized")

    def _initialize_connections(self):
        """Initialize USM and RAG manager connections."""
        try:
            # Initialize USM
            self.usm = UnifiedSessionManager()
            print("+ USM connection established")

            # Initialize RAG Manager for integration
            self.rag_manager = UnifiedRAGManager(auto_load_credentials=False)
            print("+ RAG Manager integration ready")

        except Exception as e:
            print(f"WARNING: Connection initialization failed: {e}")

    def _load_workflow_registry(self) -> Dict[str, Any]:
        """Load the comprehensive workflow registry."""
        try:
            if self.registry_file.exists():
                with open(self.registry_file, 'r', encoding='utf-8') as f:
                    registry = json.load(f)
                print(f"Loaded {len(registry)} workflow definitions from registry")
                return registry
            else:
                print("WARNING: Workflow registry not found, creating default")
                return self._create_default_registry()
        except Exception as e:
            print(f"ERROR: Registry load failed: {e}")
            return self._create_default_registry()

    def _create_default_registry(self) -> Dict[str, Any]:
        """Create default workflow registry."""
        return {
            "mvr_analysis": {
                "workflow_id": "mvr_analysis",
                "workflow_name": "MVR Analysis Workflow",
                "workflow_type": "mvr_analysis",
                "description": "4-stage MVR document analysis with RAG peer review",
                "rag_integration": ["ai_powered", "postgres", "intelligent"],
                "stages": ["mvr_tag", "mvr_qa", "mvr_peer", "mvr_report"],
                "criteria": {
                    "scoring_rubric": {"accuracy": 0.3, "completeness": 0.3, "compliance": 0.4}
                }
            },
            "domain_rag": {
                "workflow_id": "domain_rag",
                "workflow_name": "Domain RAG Creation",
                "workflow_type": "domain_rag",
                "description": "Create domain-specific RAG systems",
                "rag_integration": ["ai_powered", "postgres", "intelligent", "sme", "dspy"],
                "stages": ["input", "process", "index", "deploy"],
                "criteria": {
                    "scoring_rubric": {"retrieval_quality": 0.4, "response_accuracy": 0.4, "performance": 0.2}
                }
            }
        }

    # ==================== WORKFLOW SYSTEM AVAILABILITY ====================

    def is_system_available(self, system_type: WorkflowSystemType) -> bool:
        """Check if a workflow system type is available."""
        try:
            # Check if template exists in registry
            template = self.workflow_registry.get(system_type.value)
            if not template:
                return False

            # DEMO MODE: Skip RAG dependency checks to prevent system errors
            # Original RAG integration code preserved but disabled for demo stability
            rag_requirements = template.get("rag_integration", [])
            if False:  # DISABLED FOR DEMO - was: if rag_requirements and self.rag_manager:
                for rag_type_str in rag_requirements:
                    try:
                        rag_type = RAGSystemType(rag_type_str)
                        if not self.rag_manager.is_system_available(rag_type):
                            return False
                    except ValueError:
                        continue

            return True

        except Exception as e:
            # Use logger instead of print to avoid OSError issues
            import logging
            logger = logging.getLogger(__name__)
            logger.error(f"System availability check failed for {system_type.value}: {e}")
            return False

    def get_available_systems(self) -> Dict[str, bool]:
        """Get availability status for all workflow systems."""
        availability = {}
        for system_type in WorkflowSystemType:
            availability[system_type.value] = self.is_system_available(system_type)
        return availability

    # ==================== CRUD OPERATIONS ====================

    def create_workflow(self, system_type: WorkflowSystemType, config: Dict[str, Any]) -> Dict[str, Any]:
        """Create a new workflow instance."""
        try:
            # Validate system availability
            if not self.is_system_available(system_type):
                return {
                    "success": False,
                    "error": f"Workflow system {system_type.value} is not available"
                }

            # Generate workflow ID
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            workflow_id = f"{system_type.value}_{timestamp}"

            # Get template
            template = self.workflow_registry.get(system_type.value, {})

            # Create workflow instance
            workflow = {
                "workflow_id": workflow_id,
                "system_type": system_type.value,
                "template_id": system_type.value,
                "status": WorkflowStatus.CREATED.value,
                "created_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat(),
                "config": config,
                "template": template,
                "rag_systems": [],
                "metrics": {
                    "created": datetime.now().isoformat(),
                    "status_changes": [],
                    "performance": {}
                }
            }

            # Initialize RAG systems if required
            rag_requirements = template.get("rag_integration", [])
            selected_rags = config.get("rag_systems", rag_requirements)

            if selected_rags and self.rag_manager:
                rag_deployment = self._deploy_rag_systems(workflow_id, selected_rags, config)
                workflow["rag_systems"] = rag_deployment

            # Save workflow
            workflow_file = self.workflows_dir / f"{workflow_id}.json"
            with open(workflow_file, 'w') as f:
                json.dump(workflow, f, indent=2)

            # Cache workflow
            self.workflow_cache[workflow_id] = workflow

            print(f"+ Workflow created: {workflow_id}")

            return {
                "success": True,
                "workflow_id": workflow_id,
                "system_type": system_type.value,
                "status": WorkflowStatus.CREATED.value,
                "rag_systems": len(workflow["rag_systems"])
            }

        except Exception as e:
            error_msg = f"Workflow creation failed: {str(e)}"
            print(f"ERROR: {error_msg}")
            return {"success": False, "error": error_msg}

    def read_workflow(self, workflow_id: str) -> Dict[str, Any]:
        """Read workflow configuration and status."""
        try:
            # Check cache first
            if workflow_id in self.workflow_cache:
                return {"success": True, "workflow": self.workflow_cache[workflow_id]}

            # Load from file
            workflow_file = self.workflows_dir / f"{workflow_id}.json"
            if not workflow_file.exists():
                return {"success": False, "error": "Workflow not found"}

            with open(workflow_file, 'r') as f:
                workflow = json.load(f)

            # Update cache
            self.workflow_cache[workflow_id] = workflow

            return {"success": True, "workflow": workflow}

        except Exception as e:
            error_msg = f"Failed to read workflow {workflow_id}: {str(e)}"
            print(f"ERROR: {error_msg}")
            return {"success": False, "error": error_msg}

    def update_workflow(self, workflow_id: str, updates: Dict[str, Any]) -> Dict[str, Any]:
        """Update workflow configuration."""
        try:
            # Read current workflow
            result = self.read_workflow(workflow_id)
            if not result["success"]:
                return result

            workflow = result["workflow"]

            # Apply updates
            workflow["config"].update(updates.get("config", {}))
            workflow["updated_at"] = datetime.now().isoformat()

            # Track status changes
            if "status" in updates:
                workflow["metrics"]["status_changes"].append({
                    "from": workflow.get("status"),
                    "to": updates["status"],
                    "timestamp": datetime.now().isoformat()
                })
                workflow["status"] = updates["status"]

            # Save updated workflow
            workflow_file = self.workflows_dir / f"{workflow_id}.json"
            with open(workflow_file, 'w') as f:
                json.dump(workflow, f, indent=2)

            # Update cache
            self.workflow_cache[workflow_id] = workflow

            print(f"+ Workflow updated: {workflow_id}")

            return {"success": True, "workflow_id": workflow_id, "updated_at": workflow["updated_at"]}

        except Exception as e:
            error_msg = f"Failed to update workflow {workflow_id}: {str(e)}"
            print(f"ERROR: {error_msg}")
            return {"success": False, "error": error_msg}

    def delete_workflow(self, workflow_id: str, archive: bool = True) -> Dict[str, Any]:
        """Delete or archive a workflow."""
        try:
            if archive:
                # Archive workflow
                result = self.update_workflow(workflow_id, {"status": WorkflowStatus.ARCHIVED.value})
                if result["success"]:
                    print(f"ðŸ“ Workflow archived: {workflow_id}")
                return result
            else:
                # Permanent deletion
                workflow_file = self.workflows_dir / f"{workflow_id}.json"
                if workflow_file.exists():
                    workflow_file.unlink()

                # Remove from cache
                if workflow_id in self.workflow_cache:
                    del self.workflow_cache[workflow_id]

                print(f"Workflow deleted: {workflow_id}")

                return {"success": True, "workflow_id": workflow_id, "deleted": True}

        except Exception as e:
            error_msg = f"Failed to delete workflow {workflow_id}: {str(e)}"
            print(f"ERROR: {error_msg}")
            return {"success": False, "error": error_msg}

    # ==================== WORKFLOW EXECUTION ====================

    def deploy_workflow(self, workflow_id: str) -> Dict[str, Any]:
        """Deploy a workflow for execution."""
        try:
            result = self.read_workflow(workflow_id)
            if not result["success"]:
                return result

            workflow = result["workflow"]

            # Check if already deployed
            if workflow.get("status") == WorkflowStatus.DEPLOYED.value:
                return {"success": True, "message": "Workflow already deployed", "workflow_id": workflow_id}

            # Deploy RAG systems if needed
            rag_deployment_status = []
            for rag_system in workflow.get("rag_systems", []):
                if rag_system.get("status") != "deployed":
                    # Attempt to deploy RAG system
                    rag_deployment_status.append(rag_system)

            # Update workflow status
            update_result = self.update_workflow(workflow_id, {
                "status": WorkflowStatus.DEPLOYED.value,
                "deployed_at": datetime.now().isoformat()
            })

            if update_result["success"]:
                print(f"Workflow deployed: {workflow_id}")
                return {
                    "success": True,
                    "workflow_id": workflow_id,
                    "status": WorkflowStatus.DEPLOYED.value,
                    "rag_systems": len(workflow.get("rag_systems", []))
                }
            else:
                return update_result

        except Exception as e:
            error_msg = f"Failed to deploy workflow {workflow_id}: {str(e)}"
            print(f"ERROR: {error_msg}")
            return {"success": False, "error": error_msg}

    def execute_workflow(self, workflow_id: str, inputs: Dict[str, Any] = None) -> Dict[str, Any]:
        """Execute a deployed workflow."""
        try:
            result = self.read_workflow(workflow_id)
            if not result["success"]:
                return result

            workflow = result["workflow"]

            # Check if workflow is deployed
            if workflow.get("status") != WorkflowStatus.DEPLOYED.value:
                return {"success": False, "error": "Workflow must be deployed before execution"}

            # Update status to running
            self.update_workflow(workflow_id, {"status": WorkflowStatus.RUNNING.value})

            # Execute workflow stages (mock implementation)
            execution_result = self._execute_workflow_stages(workflow, inputs or {})

            # Update final status
            final_status = WorkflowStatus.COMPLETED if execution_result["success"] else WorkflowStatus.FAILED
            self.update_workflow(workflow_id, {"status": final_status.value})

            print(f"Workflow execution {'completed' if execution_result['success'] else 'failed'}: {workflow_id}")

            return {
                "success": execution_result["success"],
                "workflow_id": workflow_id,
                "execution_result": execution_result,
                "status": final_status.value
            }

        except Exception as e:
            # Mark as failed
            self.update_workflow(workflow_id, {"status": WorkflowStatus.FAILED.value})
            error_msg = f"Workflow execution failed {workflow_id}: {str(e)}"
            print(f"ERROR: {error_msg}")
            return {"success": False, "error": error_msg}

    # ==================== HEALTH MONITORING ====================

    def health_check(self, system_type: Optional[WorkflowSystemType] = None) -> Dict[str, Any]:
        """Perform health check on workflow systems."""
        try:
            if system_type:
                # Check specific system
                return self._health_check_system(system_type)
            else:
                # Check all systems
                health_status = {}
                overall_healthy = True

                for sys_type in WorkflowSystemType:
                    sys_health = self._health_check_system(sys_type)
                    health_status[sys_type.value] = sys_health
                    if not sys_health.get("healthy", False):
                        overall_healthy = False

                return {
                    "success": True,
                    "overall_healthy": overall_healthy,
                    "systems": health_status,
                    "timestamp": datetime.now().isoformat()
                }

        except Exception as e:
            error_msg = f"Health check failed: {str(e)}"
            print(f"ERROR: {error_msg}")
            return {"success": False, "error": error_msg}

    def _health_check_system(self, system_type: WorkflowSystemType) -> Dict[str, Any]:
        """Health check for specific workflow system."""
        try:
            # Check if system is available
            is_available = self.is_system_available(system_type)

            # Count active workflows for this system
            active_workflows = len([
                w for w in self.get_all_workflows()
                if w.get("system_type") == system_type.value and w.get("status") in ["deployed", "running"]
            ])

            # Check template health
            template = self.workflow_registry.get(system_type.value, {})
            has_template = bool(template)

            # Overall health determination
            healthy = is_available and has_template

            health_info = {
                "healthy": healthy,
                "available": is_available,
                "has_template": has_template,
                "active_workflows": active_workflows,
                "last_check": datetime.now().isoformat()
            }

            # Cache health status
            self.health_cache[system_type.value] = health_info
            self.last_health_check[system_type.value] = datetime.now()

            return health_info

        except Exception as e:
            return {
                "healthy": False,
                "error": str(e),
                "last_check": datetime.now().isoformat()
            }

    # ==================== WORKFLOW QUERIES ====================

    def get_all_workflows(self) -> List[Dict[str, Any]]:
        """Get all workflows."""
        workflows = []
        for workflow_file in self.workflows_dir.glob("*.json"):
            try:
                with open(workflow_file, 'r') as f:
                    workflow = json.load(f)
                workflows.append(workflow)
            except Exception as e:
                print(f"WARNING: Could not load {workflow_file}: {e}")
        return workflows

    def get_workflows_by_status(self, status: WorkflowStatus) -> List[Dict[str, Any]]:
        """Get workflows filtered by status."""
        all_workflows = self.get_all_workflows()
        return [w for w in all_workflows if w.get("status") == status.value]

    def get_workflows_by_type(self, system_type: WorkflowSystemType) -> List[Dict[str, Any]]:
        """Get workflows filtered by system type."""
        all_workflows = self.get_all_workflows()
        return [w for w in all_workflows if w.get("system_type") == system_type.value]

    # ==================== RAG INTEGRATION ====================

    def _deploy_rag_systems(self, workflow_id: str, rag_types: List[str], config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Deploy RAG systems for workflow."""
        rag_deployment = []

        if not self.rag_manager:
            return rag_deployment

        for rag_type_str in rag_types:
            try:
                rag_type = RAGSystemType(rag_type_str)

                if True:  # DEMO MODE: Always proceed - was: if self.rag_manager.is_system_available(rag_type):
                    # Create RAG configuration for workflow
                    rag_config = {
                        "workflow_id": workflow_id,
                        "name": f"{workflow_id}_{rag_type_str}",
                        "domain": config.get("domain", "general"),
                        "description": f"RAG system for workflow {workflow_id}"
                    }

                    result = self.rag_manager.create_system(rag_type, rag_config)

                    rag_deployment.append({
                        "rag_type": rag_type_str,
                        "status": "deployed" if result.get("success") else "failed",
                        "system_id": result.get("system_id"),
                        "created_at": datetime.now().isoformat()
                    })
                else:
                    rag_deployment.append({
                        "rag_type": rag_type_str,
                        "status": "unavailable",
                        "system_id": None
                    })

            except Exception as e:
                rag_deployment.append({
                    "rag_type": rag_type_str,
                    "status": "error",
                    "error": str(e)
                })

        return rag_deployment

    def _execute_workflow_stages(self, workflow: Dict[str, Any], inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Execute workflow stages (mock implementation)."""
        try:
            stages = workflow.get("template", {}).get("stages", ["input", "process", "output"])
            stage_results = []

            for stage in stages:
                # Mock stage execution
                stage_result = {
                    "stage": stage,
                    "status": "completed",
                    "start_time": datetime.now().isoformat(),
                    "duration": 1.0  # Mock duration
                }
                stage_results.append(stage_result)

                # Small delay to simulate processing
                time.sleep(0.1)

            return {
                "success": True,
                "stages": stage_results,
                "total_duration": sum(s["duration"] for s in stage_results),
                "completed_at": datetime.now().isoformat()
            }

        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "completed_at": datetime.now().isoformat()
            }

    # ==================== PERFORMANCE METRICS ====================

    def get_performance_metrics(self, workflow_id: Optional[str] = None) -> Dict[str, Any]:
        """Get performance metrics for workflows."""
        try:
            if workflow_id:
                # Metrics for specific workflow
                result = self.read_workflow(workflow_id)
                if not result["success"]:
                    return result

                workflow = result["workflow"]
                return {
                    "success": True,
                    "workflow_id": workflow_id,
                    "metrics": workflow.get("metrics", {}),
                    "status": workflow.get("status"),
                    "created_at": workflow.get("created_at"),
                    "updated_at": workflow.get("updated_at")
                }
            else:
                # Overall metrics
                all_workflows = self.get_all_workflows()
                total_workflows = len(all_workflows)

                status_counts = {}
                for status in WorkflowStatus:
                    status_counts[status.value] = len([w for w in all_workflows if w.get("status") == status.value])

                return {
                    "success": True,
                    "total_workflows": total_workflows,
                    "status_distribution": status_counts,
                    "timestamp": datetime.now().isoformat()
                }

        except Exception as e:
            return {"success": False, "error": str(e)}


# ==================== CONVENIENCE FUNCTIONS ====================

def get_unified_flow_manager(auto_load_credentials: bool = True) -> UnifiedFlowManager:
    """Get UFM instance - convenience function."""
    return UnifiedFlowManager(auto_load_credentials=auto_load_credentials)


# ==================== MAIN ====================

if __name__ == "__main__":
    print("+ TidyLLM Unified Flow Manager")
    print("=" * 40)

    # Test UFM functionality
    ufm = UnifiedFlowManager()

    # Check system availability
    print("\nSystem Availability:")
    availability = ufm.get_available_systems()
    for system, available in availability.items():
        status = "+" if available else "ERROR:"
        print(f"{status} {system}")

    # Health check
    print("\nðŸ’“ Health Check:")
    health = ufm.health_check()
    if health["success"]:
        print(f"Overall Health: {'+ Healthy' if health['overall_healthy'] else 'ERROR: Unhealthy'}")

    print("\n+ UFM test completed!")