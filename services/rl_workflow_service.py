"""
TidyLLM RL-Enhanced Workflow Service
====================================

Core TidyLLM service that provides RL-enhanced workflow execution capabilities.
This is sovereign TidyLLM functionality that enables intelligent workflow optimization.

Features:
- Reinforcement Learning factor optimization
- Adaptive step execution with performance feedback
- Cumulative learning across workflow executions
- Model routing based on step characteristics
- Performance tracking and trend analysis

Usage:
    from packages.tidyllm.services.rl_workflow_service import RLWorkflowService

    rl_service = RLWorkflowService(project_id="my_project")
    results = rl_service.execute_workflow_with_rl(workflow_config)
"""

from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from pathlib import Path
import json
import logging

try:
    # Import TidyLLM RL components from domain services
    import sys
    from pathlib import Path
    sys.path.append(str(Path(__file__).parent.parent.parent.parent))

    from domain.services.rl_factor_optimizer import RLFactorOptimizer
    from domain.services.cumulative_learning_pipeline import CumulativeLearningPipeline
    from domain.services.model_router_service import ModelRouterService
    from domain.services.step_attributes import BaseStep
    from domain.services.action_steps_manager import ActionStepsManager
    from domain.services.prompt_steps_manager import PromptStepsManager
    from domain.services.ask_ai_steps_manager import AskAIStepsManager
    RL_COMPONENTS_AVAILABLE = True
except ImportError as e:
    RL_COMPONENTS_AVAILABLE = False
    logging.warning(f"RL components not available: {e}")

logger = logging.getLogger(__name__)


class RLWorkflowService:
    """
    TidyLLM's core RL-enhanced workflow execution service.

    This service encapsulates all RL optimization capabilities and provides
    a clean interface for workflow execution with intelligent optimization.
    """

    def __init__(self, project_id: str):
        """Initialize the TidyLLM RL workflow service."""
        if not RL_COMPONENTS_AVAILABLE:
            raise ImportError("RL components are not available. Cannot initialize RL workflow service.")

        self.project_id = project_id
        self.session_id = f"tidyllm_rl_{project_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        # Initialize TidyLLM RL stack
        self._initialize_rl_stack()

        # Execution state
        self.workflow_results = {}
        self.step_results = []
        self.performance_metrics = {}

    def _initialize_rl_stack(self):
        """Initialize the complete TidyLLM RL optimization stack."""
        try:
            # Core RL components
            self.rl_optimizer = RLFactorOptimizer(project_id=self.project_id)
            self.learning_pipeline = CumulativeLearningPipeline(
                project_id=self.project_id,
                rl_optimizer=self.rl_optimizer
            )
            self.model_router = ModelRouterService()

            # Step management components
            self.action_manager = ActionStepsManager(self.project_id)
            self.prompt_manager = PromptStepsManager(self.project_id)
            self.ask_ai_manager = AskAIStepsManager(self.project_id)

            logger.info(f"TidyLLM RL stack initialized for project: {self.project_id}")

        except Exception as e:
            logger.error(f"Failed to initialize RL stack: {e}")
            raise

    def execute_workflow_with_rl_optimization(
        self,
        workflow_config: Dict[str, Any],
        context: Dict[str, Any] = None,
        progress_callback=None
    ) -> Dict[str, Any]:
        """
        Execute workflow with full TidyLLM RL optimization.

        Args:
            workflow_config: Complete workflow configuration
            context: Additional execution context (files, parameters, etc.)
            progress_callback: Optional function to receive progress updates

        Returns:
            Comprehensive results including RL optimization metrics
        """
        if context is None:
            context = {}

        execution_start = datetime.now()

        try:
            # Validate workflow configuration
            if not self._validate_workflow_config(workflow_config):
                return self._generate_error_response("Invalid workflow configuration")

            # Extract execution parameters
            uploaded_files = context.get('uploaded_files', [])
            field_values = context.get('field_values', {})
            execution_mode = context.get('execution_mode', 'standard')

            if progress_callback:
                progress_callback("ðŸš€ TidyLLM RL optimization engine starting")

            # Load and optimize workflow steps
            steps_config = self._load_workflow_steps(workflow_config)
            optimized_steps = self._optimize_step_sequence(steps_config)

            if progress_callback:
                progress_callback(f"ðŸ“Š Optimizing {len(optimized_steps)} workflow steps")

            # Execute workflow with RL enhancement
            execution_results = self._execute_optimized_workflow(
                optimized_steps,
                context,
                progress_callback
            )

            # Generate comprehensive results
            execution_time = (datetime.now() - execution_start).total_seconds()
            final_results = self._compile_final_results(execution_results, execution_time)

            if progress_callback:
                progress_callback("âœ… TidyLLM RL-enhanced execution complete")

            return final_results

        except Exception as e:
            logger.error(f"RL workflow execution failed: {e}")
            return self._generate_error_response(str(e))

    def _execute_optimized_workflow(
        self,
        optimized_steps: List[Dict[str, Any]],
        context: Dict[str, Any],
        progress_callback=None
    ) -> Dict[str, Any]:
        """Execute workflow steps with RL optimization."""
        execution_results = {
            'step_executions': [],
            'optimization_metrics': [],
            'learning_progression': []
        }

        for i, step_config in enumerate(optimized_steps):
            step_result = self._execute_step_with_rl_enhancement(
                step_config,
                context,
                i + 1,
                len(optimized_steps)
            )

            execution_results['step_executions'].append(step_result)

            # Update progress
            if progress_callback:
                progress = (i + 1) / len(optimized_steps)
                step_name = step_config.get('step_name', f'Step {i+1}')
                rl_reward = step_result.get('rl_metrics', {}).get('reward', 0)
                progress_callback(
                    f"Step {i+1}/{len(optimized_steps)}: {step_name} "
                    f"(RL reward: {rl_reward:.3f})",
                    progress
                )

        return execution_results

    def _execute_step_with_rl_enhancement(
        self,
        step_config: Dict[str, Any],
        context: Dict[str, Any],
        step_number: int,
        total_steps: int
    ) -> Dict[str, Any]:
        """Execute a single step with full RL enhancement."""
        step_start = datetime.now()
        step_id = step_config.get('step_id', f'step_{step_number}')
        step_name = step_config.get('step_name', f'Step {step_number}')
        step_type = step_config.get('step_type', 'process')

        try:
            # Create BaseStep for RL integration
            base_step = BaseStep(
                step_name=step_name,
                step_type=step_type,
                description=step_config.get('description', ''),
                requires=step_config.get('requires', []),
                produces=step_config.get('produces', []),
                position=step_number,
                params=step_config.get('params', {}),
                validation_rules=step_config.get('validation_rules', {}),
                kind=step_config.get('kind', step_type)
            )

            # Get RL-optimized execution factors
            rl_factors = self.rl_optimizer.optimize_step_factors(
                step_type=step_type,
                historical_data=self.workflow_results,
                context={
                    'project_id': self.project_id,
                    'session_id': self.session_id,
                    'step_position': step_number,
                    'total_steps': total_steps
                }
            )

            # Route to optimal model based on step characteristics
            optimal_model = self.model_router.route_step_to_model(base_step)
            base_step.last_routed_model = optimal_model

            # Execute step with RL-optimized parameters
            step_execution_result = self._perform_step_execution(
                step_config,
                rl_factors,
                optimal_model,
                context
            )

            # Calculate performance metrics
            execution_time = (datetime.now() - step_start).total_seconds()

            # Generate RL reward based on execution success and efficiency
            reward = self.learning_pipeline.calculate_step_reward(
                step=base_step,
                result=step_execution_result,
                execution_time=execution_time,
                success=step_execution_result.get('status') == 'success'
            )

            # Update RL learning with feedback
            self.rl_optimizer.update_with_feedback(
                step_type=step_type,
                reward=reward,
                context={
                    'execution_time': execution_time,
                    'model_used': optimal_model,
                    'result_quality': self._assess_result_quality(step_execution_result)
                }
            )

            # Update step tracking
            base_step.last_reward = reward
            base_step.last_modified = datetime.now().isoformat()

            # Store comprehensive step result
            step_result = {
                'step_id': step_id,
                'step_name': step_name,
                'step_type': step_type,
                'execution_time': execution_time,
                'status': 'success',
                'result': step_execution_result,
                'rl_metrics': {
                    'reward': reward,
                    'epsilon': rl_factors['epsilon'],
                    'learning_rate': rl_factors['learning_rate'],
                    'temperature': rl_factors['temperature'],
                    'model_used': optimal_model
                },
                'step_attributes': base_step.to_dict()
            }

            # Update workflow state
            self.workflow_results[step_id] = step_execution_result
            self.step_results.append(step_result)

            return step_result

        except Exception as e:
            # Handle step failure with RL penalty
            execution_time = (datetime.now() - step_start).total_seconds()
            failure_reward = -0.5

            self.rl_optimizer.update_with_feedback(
                step_type=step_type,
                reward=failure_reward,
                context={
                    'execution_time': execution_time,
                    'error': str(e),
                    'failure_type': 'execution_error'
                }
            )

            error_result = {
                'step_id': step_id,
                'step_name': step_name,
                'step_type': step_type,
                'execution_time': execution_time,
                'status': 'failed',
                'error': str(e),
                'rl_metrics': {
                    'reward': failure_reward,
                    'penalty_applied': True
                }
            }

            self.step_results.append(error_result)
            return error_result

    def _perform_step_execution(
        self,
        step_config: Dict[str, Any],
        rl_factors: Dict[str, Any],
        optimal_model: str,
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Perform the actual step execution with RL optimization."""
        # Placeholder for actual TidyLLM execution logic
        # In real implementation, this would call tidyllm.chat or appropriate service

        return {
            'status': 'success',
            'output': f"TidyLLM RL-optimized execution of {step_config.get('step_name')}",
            'model_used': optimal_model,
            'rl_factors_applied': rl_factors,
            'optimization_level': 'high',
            'timestamp': datetime.now().isoformat(),
            'tidyllm_service': 'rl_workflow_service'
        }

    def _assess_result_quality(self, result: Dict[str, Any]) -> float:
        """Assess the quality of step execution result."""
        # Simple quality assessment - can be enhanced
        if result.get('status') == 'success':
            output_length = len(str(result.get('output', '')))
            if output_length > 100:
                return 1.0
            elif output_length > 50:
                return 0.7
            else:
                return 0.5
        return 0.0

    def _validate_workflow_config(self, config: Dict[str, Any]) -> bool:
        """Validate workflow configuration."""
        required_fields = ['workflow_id', 'steps']
        return all(field in config for field in required_fields)

    def _load_workflow_steps(self, config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Load and prepare workflow steps."""
        steps = config.get('steps', {})
        if isinstance(steps, dict):
            return list(steps.values())
        return steps

    def _optimize_step_sequence(self, steps: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Optimize step execution sequence using RL insights."""
        # Sort by step number/position
        def sort_key(step):
            step_num = step.get('step_number', step.get('position', 0))
            if isinstance(step_num, str):
                try:
                    return [int(x) for x in step_num.split('.')]
                except:
                    return [999]
            return [int(step_num)]

        return sorted(steps, key=sort_key)

    def _compile_final_results(
        self,
        execution_results: Dict[str, Any],
        total_execution_time: float
    ) -> Dict[str, Any]:
        """Compile comprehensive final results with RL analytics."""
        try:
            # Get RL performance summaries
            rl_summary = self.rl_optimizer.get_performance_summary()
            learning_summary = self.learning_pipeline.get_cumulative_performance()

            # Calculate success metrics
            successful_steps = len([s for s in self.step_results if s['status'] == 'success'])
            total_steps = len(self.step_results)
            success_rate = successful_steps / total_steps if total_steps > 0 else 0

            # Calculate average reward
            total_reward = sum(s.get('rl_metrics', {}).get('reward', 0) for s in self.step_results)
            avg_reward = total_reward / total_steps if total_steps > 0 else 0

            return {
                'tidyllm_service': 'rl_workflow_service',
                'service_version': '1.0.0',
                'session_id': self.session_id,
                'project_id': self.project_id,
                'execution_timestamp': datetime.now().isoformat(),
                'total_execution_time': total_execution_time,
                'workflow_performance': {
                    'total_steps': total_steps,
                    'successful_steps': successful_steps,
                    'failed_steps': total_steps - successful_steps,
                    'success_rate': success_rate,
                    'average_reward': avg_reward,
                    'step_results': self.step_results
                },
                'rl_optimization': {
                    'enabled': True,
                    'optimization_level': 'advanced',
                    'performance_summary': rl_summary,
                    'learning_summary': learning_summary,
                    'cumulative_improvements': learning_summary.get('improvements', {}),
                    'trend_analysis': learning_summary.get('trend', 'stable')
                },
                'workflow_results': self.workflow_results,
                'execution_summary': execution_results
            }

        except Exception as e:
            logger.error(f"Failed to compile final results: {e}")
            return self._generate_error_response(f"Results compilation failed: {e}")

    def _generate_error_response(self, error_message: str) -> Dict[str, Any]:
        """Generate standardized error response."""
        return {
            'tidyllm_service': 'rl_workflow_service',
            'session_id': self.session_id,
            'project_id': self.project_id,
            'execution_timestamp': datetime.now().isoformat(),
            'status': 'error',
            'error': error_message,
            'rl_optimization': {'enabled': True, 'error': error_message}
        }

    def get_service_info(self) -> Dict[str, Any]:
        """Get information about this TidyLLM service."""
        return {
            'service_name': 'rl_workflow_service',
            'service_type': 'tidyllm_core_service',
            'version': '1.0.0',
            'capabilities': [
                'rl_workflow_optimization',
                'adaptive_step_execution',
                'performance_learning',
                'model_routing',
                'cumulative_improvement'
            ],
            'project_id': self.project_id,
            'session_id': self.session_id,
            'rl_components_available': RL_COMPONENTS_AVAILABLE
        }